{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bada8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Event Detection Average Precision\n",
    "\n",
    "An average precision metric for event detection in time series and\n",
    "video.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Set some placeholders for global parameters\n",
    "series_id_column_name = None\n",
    "time_column_name = None\n",
    "event_column_name = None\n",
    "score_column_name = None\n",
    "use_scoring_intervals = None\n",
    "\n",
    "\n",
    "def score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    "        series_id_column_name: str,\n",
    "        time_column_name: str,\n",
    "        event_column_name: str,\n",
    "        score_column_name: str,\n",
    "        use_scoring_intervals: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"Event Detection Average Precision, an AUCPR metric for event detection in\n",
    "    time series and video.\n",
    "\n",
    "    This metric is similar to IOU-threshold average precision metrics commonly\n",
    "    used in object detection. For events occuring in time series, we replace the\n",
    "    IOU threshold with a time tolerance.\n",
    "\n",
    "    Submissions are evaluated on the average precision of detected events,\n",
    "    averaged over timestamp error tolerance thresholds, averaged over event\n",
    "    classes.\n",
    "\n",
    "    Detections are matched to ground-truth events within error tolerances, with\n",
    "    ambiguities resolved in order of decreasing confidence.\n",
    "\n",
    "    Detailed Description\n",
    "    --------------------\n",
    "    Evaluation proceeds in four steps:\n",
    "\n",
    "    1. Selection - (optional) Predictions not within a series' scoring\n",
    "    intervals are dropped.\n",
    "    2. Assignment - Predicted events are matched with ground-truth events.\n",
    "    3. Scoring - Each group of predictions is scored against its corresponding\n",
    "    group of ground-truth events via Average Precision.\n",
    "    4. Reduction - The multiple AP scores are averaged to produce a single\n",
    "    overall score.\n",
    "\n",
    "    Selection\n",
    "\n",
    "    With each series there may be a defined set of scoring intervals giving the\n",
    "    intervals of time over which zero or more ground-truth events might be\n",
    "    annotated in that series. A prediction will be evaluated only if it falls\n",
    "    within a scoring interval. These scoring intervals can be chosen to improve\n",
    "    the fairness of evaluation by, for instance, ignoring edge-cases or\n",
    "    ambiguous events.\n",
    "\n",
    "    It is recommended that, if used, scoring intervals be provided for training\n",
    "    data but not test data.\n",
    "\n",
    "    Assignment\n",
    "\n",
    "    For each set of predictions and ground-truths within the same `event x\n",
    "    tolerance x series_id` group, we match each ground-truth to the\n",
    "    highest-confidence unmatched prediction occurring within the allowed\n",
    "    tolerance.\n",
    "\n",
    "    Some ground-truths may not be matched to a prediction and some predictions\n",
    "    may not be matched to a ground-truth. They will still be accounted for in\n",
    "    the scoring, however.\n",
    "\n",
    "    Scoring\n",
    "\n",
    "    Collecting the events within each `series_id`, we compute an Average\n",
    "    Precision score for each `event x tolerance` group. The average precision\n",
    "    score is the area under the (step-wise) precision-recall curve generated by\n",
    "    decreasing confidence score thresholds over the predictions. In this\n",
    "    calculation, matched predictions over the threshold are scored as TP and\n",
    "    unmatched predictions as FP. Unmatched ground-truths are scored as FN.\n",
    "\n",
    "    Reduction\n",
    "\n",
    "    The final score is the average of the above AP scores, first averaged over\n",
    "    tolerance, then over event.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : pd.DataFrame, with columns:\n",
    "\n",
    "        `series_id_column_name` identifier for each time series\n",
    "\n",
    "        `time_column_name` the time of occurence for each event as a numeric type\n",
    "\n",
    "        `event_column_name` class label for each event\n",
    "\n",
    "        The solution contains the time of occurence of one or more types of\n",
    "        event within one or more time series. The metric expects the solution to\n",
    "        contain the same event types as those given in `tolerances`.\n",
    "\n",
    "        When `use_scoring_intervals == True`, you may include `start` and `end`\n",
    "        events to delimit intervals within which detections will be scored.\n",
    "        Detected events (from the user submission) outside of these events will\n",
    "        be ignored.\n",
    "\n",
    "    submission : pd.DataFrame, with columns as above and in addition:\n",
    "\n",
    "        `score_column_name` the predicted confidence score for the detected event\n",
    "\n",
    "    tolerances : Dict[str, List[float]]\n",
    "\n",
    "        Maps each event class to a list of timestamp tolerances used\n",
    "        for matching detections to ground-truth events.\n",
    "\n",
    "    use_scoring_intervals: bool, default False\n",
    "\n",
    "        Whether to ignore predicted events outside intervals delimited\n",
    "        by `'start'` and `'end'` events in the solution. When `False`,\n",
    "        the solution should not include `'start'` and `'end'` events.\n",
    "        See the examples for illustration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    event_detection_ap : float\n",
    "        The mean average precision of the detected events.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Detecting `'pass'` events in football:\n",
    "    >>> column_names = {\n",
    "    ...     'series_id_column_name': 'video_id',\n",
    "    ...     'time_column_name': 'time',\n",
    "    ...     'event_column_name': 'event',\n",
    "    ...     'score_column_name': 'score',\n",
    "    ... }\n",
    "    >>> tolerances = {'pass': [1.0]}\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'video_id': ['a', 'a'],\n",
    "    ...     'event': ['pass', 'pass'],\n",
    "    ...     'time': [0, 15],\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'video_id': ['a', 'a', 'a'],\n",
    "    ...     'event': ['pass', 'pass', 'pass'],\n",
    "    ...     'score': [1.0, 0.5, 1.0],\n",
    "    ...     'time': [0, 10, 14.5],\n",
    "    ... })\n",
    "    >>> score(solution, submission, tolerances, **column_names)\n",
    "    1.0\n",
    "\n",
    "    Increasing the confidence score of the false detection above the true\n",
    "    detections decreases the AP.\n",
    "    >>> submission.loc[1, 'score'] = 1.5\n",
    "    >>> score(solution, submission, tolerances, **column_names)\n",
    "    0.6666666666666666...\n",
    "\n",
    "    Likewise, decreasing the confidence score of a true detection below the\n",
    "    false detection also decreases the AP.\n",
    "    >>> submission.loc[1, 'score'] = 0.5  # reset\n",
    "    >>> submission.loc[0, 'score'] = 0.0\n",
    "    >>> score(solution, submission, tolerances, **column_names)\n",
    "    0.8333333333333333...\n",
    "\n",
    "    We average AP scores over tolerances. Previously, the detection at 14.5\n",
    "    would match, but adding smaller tolerances gives AP scores where it does\n",
    "    not match. This results in both a FN, since the ground-truth wasn't\n",
    "    detected, and a FP, since the detected event matches no ground-truth.\n",
    "    >>> tolerances = {'pass': [0.1, 0.2, 1.0]}\n",
    "    >>> score(solution, submission, tolerances, **column_names)\n",
    "    0.3888888888888888...\n",
    "\n",
    "    We also average over time series and over event classes.\n",
    "    >>> tolerances = {'pass': [0.5, 1.0], 'challenge': [0.25, 0.50]}\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'video_id': ['a', 'a', 'b'],\n",
    "    ...     'event': ['pass', 'challenge', 'pass'],\n",
    "    ...     'time': [0, 15, 0],  # restart time for new time series b\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'video_id': ['a', 'a', 'b'],\n",
    "    ...     'event': ['pass', 'challenge', 'pass'],\n",
    "    ...     'score': [1.0, 0.5, 1.0],\n",
    "    ...     'time': [0, 15, 0],\n",
    "    ... })\n",
    "    >>> score(solution, submission, tolerances, **column_names)\n",
    "    1.0\n",
    "\n",
    "    By adding scoring intervals to the solution, we may choose to ignore\n",
    "    detections outside of those intervals.\n",
    "    >>> tolerances = {'pass': [1.0]}\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'video_id': ['a', 'a', 'a', 'a'],\n",
    "    ...     'event': ['start', 'pass', 'pass', 'end'],\n",
    "    ...     'time': [0, 10, 20, 30],\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'video_id': ['a', 'a', 'a'],\n",
    "    ...     'event': ['pass', 'pass', 'pass'],\n",
    "    ...     'score': [1.0, 1.0, 1.0],\n",
    "    ...     'time': [10, 20, 40],\n",
    "    ... })\n",
    "    >>> score(solution, submission, tolerances, **column_names, use_scoring_intervals=True)\n",
    "    1.0\n",
    "\n",
    "    \"\"\"\n",
    "    # Validate metric parameters\n",
    "    assert len(tolerances) > 0, \"Events must have defined tolerances.\"\n",
    "    assert set(tolerances.keys()) == set(solution[event_column_name]).difference({'start', 'end'}),\\\n",
    "        (f\"Solution column {event_column_name} must contain the same events \"\n",
    "         \"as defined in tolerances.\")\n",
    "    assert pd.api.types.is_numeric_dtype(solution[time_column_name]),\\\n",
    "        f\"Solution column {time_column_name} must be of numeric type.\"\n",
    "\n",
    "    # Validate submission format\n",
    "    for column_name in [\n",
    "        series_id_column_name,\n",
    "        time_column_name,\n",
    "        event_column_name,\n",
    "        score_column_name,\n",
    "    ]:\n",
    "        if column_name not in submission.columns:\n",
    "            raise ParticipantVisibleError(f\"Submission must have column '{target_name}'.\")\n",
    "\n",
    "    if not pd.api.types.is_numeric_dtype(submission[time_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{time_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "    if not pd.api.types.is_numeric_dtype(submission[score_column_name]):\n",
    "        raise ParticipantVisibleError(\n",
    "            f\"Submission column '{score_column_name}' must be of numeric type.\"\n",
    "        )\n",
    "\n",
    "    # Set these globally to avoid passing around a bunch of arguments\n",
    "    globals()['series_id_column_name'] = series_id_column_name\n",
    "    globals()['time_column_name'] = time_column_name\n",
    "    globals()['event_column_name'] = event_column_name\n",
    "    globals()['score_column_name'] = score_column_name\n",
    "    globals()['use_scoring_intervals'] = use_scoring_intervals\n",
    "\n",
    "    return event_detection_ap(solution, submission, tolerances)\n",
    "\n",
    "\n",
    "def filter_detections(\n",
    "        detections: pd.DataFrame, intervals: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop detections not inside a scoring interval.\"\"\"\n",
    "    detection_time = detections.loc[:, time_column_name].sort_values().to_numpy()\n",
    "    intervals = intervals.to_numpy()\n",
    "    is_scored = np.full_like(detection_time, False, dtype=bool)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < len(detection_time) and j < len(intervals):\n",
    "        time = detection_time[i]\n",
    "        int_ = intervals[j]\n",
    "\n",
    "        # If the detection is prior in time to the interval, go to the next detection.\n",
    "        if time < int_.left:\n",
    "            i += 1\n",
    "        # If the detection is inside the interval, keep it and go to the next detection.\n",
    "        elif time in int_:\n",
    "            is_scored[i] = True\n",
    "            i += 1\n",
    "        # If the detection is later in time, go to the next interval.\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return detections.loc[is_scored].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def match_detections(\n",
    "        tolerance: float, ground_truths: pd.DataFrame, detections: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Match detections to ground truth events. Arguments are taken from a common event x tolerance x series_id evaluation group.\"\"\"\n",
    "    detections_sorted = detections.sort_values(score_column_name, ascending=False).dropna()\n",
    "    is_matched = np.full_like(detections_sorted[event_column_name], False, dtype=bool)\n",
    "    gts_matched = set()\n",
    "    for i, det in enumerate(detections_sorted.itertuples(index=False)):\n",
    "        best_error = tolerance\n",
    "        best_gt = None\n",
    "\n",
    "        for gt in ground_truths.itertuples(index=False):\n",
    "            error = abs(getattr(det, time_column_name) - getattr(gt, time_column_name))\n",
    "            if error < best_error and gt not in gts_matched:\n",
    "                best_gt = gt\n",
    "                best_error = error\n",
    "\n",
    "        if best_gt is not None:\n",
    "            is_matched[i] = True\n",
    "            gts_matched.add(best_gt)\n",
    "\n",
    "    detections_sorted['matched'] = is_matched\n",
    "\n",
    "    return detections_sorted\n",
    "\n",
    "\n",
    "def precision_recall_curve(\n",
    "        matches: np.ndarray, scores: np.ndarray, p: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    if len(matches) == 0:\n",
    "        return [1], [0], []\n",
    "\n",
    "    # Sort matches by decreasing confidence\n",
    "    idxs = np.argsort(scores, kind='stable')[::-1]\n",
    "    scores = scores[idxs]\n",
    "    matches = matches[idxs]\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(scores))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, matches.size - 1]\n",
    "    thresholds = scores[threshold_idxs]\n",
    "\n",
    "    # Matches become TPs and non-matches FPs as confidence threshold decreases\n",
    "    tps = np.cumsum(matches)[threshold_idxs]\n",
    "    fps = np.cumsum(~matches)[threshold_idxs]\n",
    "\n",
    "    precision = tps / (tps + fps)\n",
    "    precision[np.isnan(precision)] = 0\n",
    "    recall = tps / p  # total number of ground truths might be different than total number of matches\n",
    "\n",
    "    # Stop when full recall attained and reverse the outputs so recall is non-increasing.\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "\n",
    "    # Final precision is 1 and final recall is 0\n",
    "    return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]\n",
    "\n",
    "\n",
    "def average_precision_score(matches: np.ndarray, scores: np.ndarray, p: int) -> float:\n",
    "    precision, recall, _ = precision_recall_curve(matches, scores, p)\n",
    "    # Compute step integral\n",
    "    return -np.sum(np.diff(recall) * np.array(precision)[:-1])\n",
    "\n",
    "\n",
    "def event_detection_ap(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        tolerances: Dict[str, List[float]],\n",
    ") -> float:\n",
    "\n",
    "    # Ensure solution and submission are sorted properly\n",
    "    solution = solution.sort_values([series_id_column_name, time_column_name])\n",
    "    submission = submission.sort_values([series_id_column_name, time_column_name])\n",
    "\n",
    "    # Extract scoring intervals.\n",
    "    if use_scoring_intervals:\n",
    "        intervals = (\n",
    "            solution\n",
    "            .query(\"event in ['start', 'end']\")\n",
    "            .assign(interval=lambda x: x.groupby([series_id_column_name, event_column_name]).cumcount())\n",
    "            .pivot(\n",
    "                index='interval',\n",
    "                columns=[series_id_column_name, event_column_name],\n",
    "                values=time_column_name,\n",
    "            )\n",
    "            .stack(series_id_column_name)\n",
    "            .swaplevel()\n",
    "            .sort_index()\n",
    "            .loc[:, ['start', 'end']]\n",
    "            .apply(lambda x: pd.Interval(*x, closed='both'), axis=1)\n",
    "        )\n",
    "\n",
    "    # Extract ground-truth events.\n",
    "    ground_truths = (\n",
    "        solution\n",
    "        .query(\"event not in ['start', 'end']\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Map each event class to its prevalence (needed for recall calculation)\n",
    "    class_counts = ground_truths.value_counts(event_column_name).to_dict()\n",
    "\n",
    "    # Create table for detections with a column indicating a match to a ground-truth event\n",
    "    detections = submission.assign(matched = False)\n",
    "\n",
    "    # Remove detections outside of scoring intervals\n",
    "    if use_scoring_intervals:\n",
    "        detections_filtered = []\n",
    "        for (det_group, dets), (int_group, ints) in zip(\n",
    "            detections.groupby(series_id_column_name), intervals.groupby(series_id_column_name)\n",
    "        ):\n",
    "            assert det_group == int_group\n",
    "            detections_filtered.append(filter_detections(dets, ints))\n",
    "        detections_filtered = pd.concat(detections_filtered, ignore_index=True)\n",
    "    else:\n",
    "        detections_filtered = detections\n",
    "\n",
    "    # Create table of event-class x tolerance x series_id values\n",
    "    aggregation_keys = pd.DataFrame(\n",
    "        [(ev, tol, vid)\n",
    "         for ev in tolerances.keys()\n",
    "         for tol in tolerances[ev]\n",
    "         for vid in ground_truths[series_id_column_name].unique()],\n",
    "        columns=[event_column_name, 'tolerance', series_id_column_name],\n",
    "    )\n",
    "\n",
    "    # Create match evaluation groups: event-class x tolerance x series_id\n",
    "    detections_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(detections_filtered, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    ground_truths_grouped = (\n",
    "        aggregation_keys\n",
    "        .merge(ground_truths, on=[event_column_name, series_id_column_name], how='left')\n",
    "        .groupby([event_column_name, 'tolerance', series_id_column_name])\n",
    "    )\n",
    "    # Match detections to ground truth events by evaluation group\n",
    "    detections_matched = []\n",
    "    for key in tqdm(aggregation_keys.itertuples(index=False)):\n",
    "        dets = detections_grouped.get_group(key)\n",
    "        gts = ground_truths_grouped.get_group(key)\n",
    "        detections_matched.append(\n",
    "            match_detections(dets['tolerance'].iloc[0], gts, dets)\n",
    "        )\n",
    "    detections_matched = pd.concat(detections_matched)\n",
    "\n",
    "    # Compute AP per event x tolerance group\n",
    "    event_classes = ground_truths[event_column_name].unique()\n",
    "    ap_table = (\n",
    "        detections_matched\n",
    "        .query(\"event in @event_classes\")\n",
    "        .groupby([event_column_name, 'tolerance']).apply(\n",
    "            lambda group: average_precision_score(\n",
    "                group['matched'].to_numpy(),\n",
    "                group[score_column_name].to_numpy(),\n",
    "                class_counts[group[event_column_name].iat[0]],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    # Average over tolerances, then over event classes\n",
    "    mean_ap = ap_table.groupby(event_column_name).mean().sum() / len(event_classes)\n",
    "\n",
    "    return mean_ap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
